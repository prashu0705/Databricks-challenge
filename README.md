# Databricks-challenge
# Databricks Learning Journey â€“ Day 1

**Date:** 09/01/2026  

## Overview
This repository documents my learning journey with **Databricks and PySpark**, focusing on understanding modern data engineering platforms through hands-on practice.

---

## Day 1: Platform Setup & First Steps

### Topics Learned
- Why Databricks compared to Pandas and Hadoop  
- Fundamentals of Lakehouse architecture  
- Databricks workspace structure (Workspace, Compute, Data Explorer)  
- Industry use cases (Netflix, Shell, Comcast)  

---

## Hands-on Tasks Completed
1. Created a Databricks Community Edition account  
2. Explored Workspace, Compute, and Data Explorer  
3. Created the first Databricks notebook  
4. Executed basic PySpark commands  

---
Key Takeaways
Databricks provides an integrated Lakehouse platform combining data engineering and analytics
PySpark enables scalable data transformations using lazy evaluation
Databricks notebooks offer an interactive and efficient development environment
Tools & Technologies
Databricks Community Edition
PySpark
Learning References & Acknowledgements
This learning initiative is supported by the data community and learning resources from:
Databricks â€” https://www.databricks.com
Codebasics â€” https://www.codebasics.io
Indian Data Club â€” https://indiandataclub.com

## Day 2 â€“ Apache Spark Fundamentals ðŸš€

Today I explored the fundamentals of Apache Spark using Databricks Community Edition.

### What I learned:
- Spark architecture (Driver, Executors, DAG)
- Difference between DataFrames and RDDs
- Lazy evaluation in Spark
- Using Databricks magic commands (%python, %sql, %fs)

### What I built:
- Loaded an e-commerce CSV dataset into Spark
- Performed select, filter, groupBy, and orderBy operations
- Aggregated revenue by product category
- Exported processed data back to storage

Apache Spark makes large-scale data processing both powerful and intuitive when using DataFrames.

Thanks to the learning initiative by @Databricks, @Codebasics, and @indiandataclub ðŸ™Œ  
#DatabricksWithIDC

## Day 3 â€“ Advanced Spark Analytics ðŸš€  
ðŸ“… 11/01/26

Today I moved beyond basic Spark operations and worked on **advanced analytics patterns used in real-world data engineering and analytics workflows**.

### ðŸ” What I learned:
- Performing **complex joins** between raw event data and aggregated metrics
- Using **window functions** to calculate running totals
- Creating **derived features** for analytics and ML use cases
- Understanding how Spark optimizes these operations using DAGs and lazy evaluation

---

### ðŸ› ï¸ What I implemented:

#### 1ï¸âƒ£ Complex Joins
- Joined raw user activity data with user-level purchase aggregates  
- Used LEFT JOINs to preserve non-purchasing users  
- Typical use case: customer analytics and behavioral segmentation

#### 2ï¸âƒ£ Window Functions
- Calculated **running (cumulative) spend per user over time**
- Used `partitionBy` and `orderBy` to track user-level temporal behavior  
- Common in churn analysis, fraud detection, and growth analytics

#### 3ï¸âƒ£ Derived Features
Engineered meaningful features such as:
- Total spend per user  
- Average spend per purchase  
- High-value user flag  
- Price bucket classification (LOW / MEDIUM / HIGH)

These features are directly usable for:
- Dashboards
- Business insights
- Downstream ML models

---

### âœ… Key Takeaways:
- Spark window functions enable powerful time-based analytics at scale
- Feature engineering is a critical step between raw data and ML
- DataFrames provide both performance and readability for complex logic
- Spark transformations remain lazy until an action is triggered

---

### ðŸ”§ Tech Stack:
- Apache Spark (PySpark)
- Databricks Community Edition
- Kaggle E-commerce Dataset

Thanks to the learning initiative by  
@Databricks @Codebasics @indiandataclub  

#DatabricksWithIDC

## Day 4 â€“ Delta Lake & Data Reliability (Unity Catalog) ðŸš€  
ðŸ“… 12/01/26

Today I worked on **Delta Lake fundamentals with Unity Catalog**, focusing on how Databricks ensures data reliability, schema enforcement, and safe data ingestion in production environments.

This day was especially insightful because it exposed **real-world constraints and errors** that data engineers face while working with Delta + UC.

---

### ðŸ” What I learned:

#### 1ï¸âƒ£ CSV â†’ Delta Conversion
- Converted raw e-commerce CSV data into **Delta format**
- Understood why Delta is preferred over CSV/Parquet for analytics pipelines

#### 2ï¸âƒ£ Delta Tables with Unity Catalog
- Created **managed Delta tables** in Unity Catalog
- Learned why `dbfs:/` and `file:/` paths are **not supported in UC**
- Used **Volumes and managed tables** as UC-compliant storage options

#### 3ï¸âƒ£ Schema Enforcement
- Verified that Delta **rejects invalid schema writes**
- Observed how Delta prevents bad data ingestion at write time
- Understood the difference between schema inference vs schema enforcement

#### 4ï¸âƒ£ Handling Duplicate Inserts
- Detected duplicate records using business keys
- Implemented de-duplication strategies
- Used **MERGE INTO** for idempotent writes and safe upserts

#### 5ï¸âƒ£ Practical Debugging (Real Engineering Lessons)
- Resolved:
  - Missing filesystem scheme errors
  - Unity Catalog path restrictions
  - `_c0`, `_c1` column issues caused by missing CSV headers
- Learned why proper schema definition is critical before using MERGE

---

### ðŸ§  Key Takeaways:
- Delta Lake adds **ACID guarantees** to data lakes
- Unity Catalog enforces **strict governance and storage rules**
- Managed tables are often safer than external locations
- MERGE operations require **clean schemas with correct column names**
- Many Delta errors are not bugs, but **design safeguards**

---

### ðŸ”§ Tech Stack:
- Apache Spark (PySpark)
- Delta Lake
- Databricks Community Edition
- Unity Catalog
- Kaggle E-commerce Behavior Dataset

---

Thanks to the learning initiative by  
@Databricks @Codebasics @indiandataclub  

#DatabricksWithIDC

Day 5 â€“ Delta Lake Advanced Operations (Databricks)
Todayâ€™s focus was on managing incremental data, performance optimization, and storage hygiene using Delta Lake. These are critical concepts for building production-grade Lakehouse pipelines.
âœ… What I Learned & Implemented
1ï¸âƒ£ Incremental Data Handling with MERGE
Implemented upserts using Delta Lake MERGE
Efficiently handled new and updated records
Prevented duplicate data during incremental loads
Key use case: CDC pipelines and Bronze â†’ Silver transformations
2ï¸âƒ£ Querying Historical Data (Time Travel)
Queried previous versions of Delta tables
Used version-based and timestamp-based queries
Understood how Delta maintains transactional history
Useful for auditing, debugging, and rollback scenarios
3ï¸âƒ£ Performance Optimization with OPTIMIZE
Compacted small files into larger ones
Used Z-Ordering on frequently filtered columns
Improved query latency and scan efficiency
Crucial for large analytical tables
4ï¸âƒ£ Storage Cleanup with VACUUM
Removed obsolete files created by updates and merges
Learned retention-period best practices
Understood trade-offs between cleanup and time travel
ðŸ§  Key Takeaway
Delta Lake is not just about storing data â€” it enables reliable incremental processing, auditability, high performance, and efficient storage management, all within the Lakehouse architecture.
ðŸ› ï¸ Tools & Concepts
Databricks Community Edition
Delta Lake
MERGE (Upserts)
Time Travel
OPTIMIZE & Z-ORDER
VACUUM
ðŸ“Œ Building strong foundations for scalable data engineering and analytics pipelines.
#Databricks #DeltaLake #Lakehouse #DataEngineering #BigData #LearningInPublic
ðŸ“… Day 6 â€“ Medallion Architecture (Bronze â†’ Silver â†’ Gold)
ðŸ“˜ What I Learned
Todayâ€™s focus was on understanding and implementing the Medallion Architecture, a core design pattern in modern data lakehouse systems.
Bronze Layer (Raw)
Stores raw, immutable data exactly as received from source systems.
Silver Layer (Cleaned)
Applies data cleaning, validation, deduplication, and schema enforcement.
Gold Layer (Aggregated)
Contains business-ready datasets optimized for analytics and reporting.
This layered approach improves data quality, scalability, auditability, and performance, while enabling incremental and reliable data pipelines.
ðŸ› ï¸ Hands-on Tasks Completed
Designed a 3-layer Lakehouse architecture
Clearly defined responsibilities for Bronze, Silver, and Gold layers
Followed separation-of-concerns best practices
Built Bronze Layer â€“ Raw Ingestion
Ingested raw data as-is
Preserved original schema and timestamps
Ensured append-only, immutable storage
Built Silver Layer â€“ Cleaning & Validation
Removed duplicates and invalid records
Standardized data formats
Enforced schema consistency
Built Gold Layer â€“ Business Aggregates
Created analytical aggregates for downstream use
Optimized tables for reporting and BI queries
ðŸ’¡ Key Takeaways
Medallion architecture simplifies pipeline maintenance
Incremental processing reduces compute cost and improves efficiency
Clear data layering makes debugging and governance easier
ðŸš€ Tech Stack
Apache Spark
Delta Lake
Databricks Community Edition
ðŸ“Œ Continuing to document my Databricks & Lakehouse learning journey step by step.
Looking forward to implementing more advanced incremental and optimization patterns next.
#Databricks #Lakehouse #DeltaLake #DataEngineering #MedallionArchitecture #LearningInPublic

DAY 7 (15/01/26) â€” Workflows & Job Orchestration (Databricks)
Todayâ€™s focus was on orchestrating end-to-end data pipelines using Databricks Jobs and Workflows, moving from standalone notebooks to production-grade automated pipelines.
ðŸ“˜ What I Learned
Difference between Databricks Notebooks vs Jobs
Designing multi-task workflows
Using parameters & widgets for dynamic execution
Setting up task dependencies
Basics of error handling & retries
Scheduling pipelines for automated runs
ðŸ› ï¸ Hands-on Tasks Completed
Added parameter widgets to notebooks for reusable execution
Built a multi-task Databricks Job following the Medallion Architecture
Bronze â†’ Silver â†’ Gold
Configured task dependencies to ensure correct execution order
Scheduled the workflow for automatic execution
Validated successful orchestration and task-level status monitoring
ðŸ—ï¸ Architecture Implemented
Bronze (Raw Ingestion)
        â†“
Silver (Cleaning & Validation)
        â†“
Gold (Business Aggregations)
Each layer runs as an independent task, orchestrated using Databricks Workflows.
ðŸš€ Key Takeaway
Databricks Workflows enable scalable, fault-tolerant, and automated data pipelines, making them suitable for real-world production use cases beyond exploratory notebooks.
ðŸ”— Repository
ðŸ“Œ Code and workflow configurations are available in this repository.
ðŸ”– Tags
#Databricks #DataEngineering #Workflows #JobOrchestration
#MedallionArchitecture #BigData #Lakehouse

## ðŸ“… Day 8 â€“ Data Governance & Catalog Management (Databricks 14 Days AI Challenge)

### ðŸ” What I Learned

* **Catalog â†’ Schema â†’ Table hierarchy** and how Databricks logically organizes data assets
* Basics of **data governance** and why access control is critical in analytics platforms
* Conceptual understanding of **GRANT / REVOKE** permissions
* Difference between **managed tables** and **external tables**
* How **views** can be used to expose controlled, business-ready data
* Importance of **data lineage** in understanding data flow across layers

---

### ðŸ› ï¸ What I Implemented

#### 1ï¸âƒ£ Catalog & Schema Structure

In Databricks Community Edition, I used the default catalog (`workspace`) and created a dedicated schema for the project:

```sql
CREATE SCHEMA IF NOT EXISTS workspace.ecommerce;
```

This schema represents a logical boundary for all e-commerce analytics tables.

---

#### 2ï¸âƒ£ Registering Delta Tables

I registered **managed Delta tables** created from the Medallion Architecture:

* `bronze_events` â€“ raw ingested data
* `silver_events` â€“ cleaned and deduplicated data
* `gold_product_metrics` â€“ aggregated business metrics

These tables are managed by Databricks, with storage and metadata handled automatically.

```sql
SHOW TABLES IN workspace.ecommerce;
```

---

#### 3ï¸âƒ£ Access Control (Conceptual)

While Databricks Community Edition does not support `GRANT` / `REVOKE`, I designed access logically:

* **Data Engineers** â†’ Bronze & Silver tables
* **Analysts / Business Users** â†’ Gold tables or curated views only

This mirrors real-world governance practices used with Unity Catalog in production environments.

---

#### 4ï¸âƒ£ Controlled Access Using Views

To expose only analytics-ready data, I created a curated **Gold-layer view**:

```sql
CREATE VIEW workspace.ecommerce.top_products AS
SELECT
  product_id,
  total_purchases,
  total_revenue
FROM workspace.ecommerce.gold_product_metrics
WHERE total_purchases > 10
ORDER BY total_revenue DESC;
```

This ensures:

* No access to raw or sensitive data
* Consistent business logic
* Simplified querying for analysts

---

### ðŸ§  Key Takeaways

* Governance is as important as data processing in production systems
* Views are a powerful way to enforce **controlled access**
* Managed tables simplify storage and lifecycle management
* Unity Catalog concepts can be **understood and applied logically** even in Community Edition

---

### âœ… Outcome

* Implemented catalog & schema structure
* Registered Delta tables
* Designed access control strategy
* Created governed views for analytics

ðŸ“Œ **Day 8 complete â€” ready for production-grade data governance concepts.**

---

#Databricks #DataGovernance #UnityCatalog #DeltaLake #MedallionArchitecture #DatabricksWithIDC

